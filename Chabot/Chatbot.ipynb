{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tm5g4WIG5ym2"
   },
   "source": [
    "## 1) Importing the packages\n",
    "\n",
    "We will import [TensorFlow](https://www.tensorflow.org) and our beloved [Keras](https://www.tensorflow.org/guide/keras). Also, we import other modules which help in defining model layers.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UgZHR8TO0lFF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxiGOLldKOQD"
   },
   "source": [
    "## 2) Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nF1mDKD_R6Os"
   },
   "source": [
    "### B) Reading the data from the files\n",
    "\n",
    "We parse each of the `.yaml` files.\n",
    "\n",
    "*   Concatenate two or more sentences if the answer has two or more of them.\n",
    "*   Remove unwanted data types which are produced while parsing the data.\n",
    "*   Append `<START>` and `<END>` to all the `answers`.\n",
    "*   Create a `Tokenizer` and load the whole vocabulary ( `questions` + `answers` ) into it.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5006,
     "status": "error",
     "timestamp": 1586291953299,
     "user": {
      "displayName": "Romulo Mello",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi_N9tPpV0djdlpTt9-q7WPX-2WAIq92Z-t6CYv6g=s64",
      "userId": "18236081457858034616"
     },
     "user_tz": 180
    },
    "id": "RzTBhga6MiV7",
    "outputId": "1f066bf4-a6ed-4975-926a-164e46c4fc03"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f29208f79438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/conversations.yml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mconversations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conversations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/conversations.yml'"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras import preprocessing , utils\n",
    "import os,re\n",
    "import yaml\n",
    "\n",
    "questions = list()\n",
    "answers = list()\n",
    "\n",
    "\n",
    "stream = open('conversations.yml', 'rb')\n",
    "docs = yaml.safe_load(stream)\n",
    "conversations = docs['conversations']\n",
    "\n",
    "questions_test = []\n",
    "answers_test = []\n",
    "num_lists = 0\n",
    "\n",
    "for con in conversations:\n",
    "  num_lists = num_lists + 1  \n",
    "\n",
    "for i in range(0,num_lists):\n",
    "  for x in range(len(conversations[i])):\n",
    "    if len(conversations[i]) % 2 == 0:\n",
    "      if x % 2 == 0:\n",
    "        questions.append(conversations[i][x])\n",
    "      else:\n",
    "        answers.append('<START>' + conversations[i][x] + \"<END>\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1268,
     "status": "ok",
     "timestamp": 1586196382853,
     "user": {
      "displayName": "Thales Fonseca",
      "photoUrl": "",
      "userId": "01438805462048937481"
     },
     "user_tz": 180
    },
    "id": "nqz4GjLbZ1vx",
    "outputId": "23f8bfa8-c95c-4766-d788-a78575ab2ee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 242\n"
     ]
    }
   ],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( questions + answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "word2idx = tokenizer.word_index\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzsaO1YvS-M8"
   },
   "source": [
    "\n",
    "### C) Preparing data for Seq2Seq model\n",
    "\n",
    "Our model requires three arrays namely `encoder_input_data`, `decoder_input_data` and `decoder_output_data`.\n",
    "\n",
    "For `encoder_input_data` :\n",
    "* Tokenize the `questions`. Pad them to their maximum length.\n",
    "\n",
    "For `decoder_input_data` :\n",
    "* Tokenize the `answers`. Pad them to their maximum length.\n",
    "\n",
    "For `decoder_output_data` :\n",
    "\n",
    "* Tokenize the `answers`. Remove the first element from all the `tokenized_answers`. This is the `<START>` element which we added earlier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27531,
     "status": "ok",
     "timestamp": 1586196414242,
     "user": {
      "displayName": "Thales Fonseca",
      "photoUrl": "",
      "userId": "01438805462048937481"
     },
     "user_tz": 180
    },
    "id": "a5AD9ooQKc33",
    "outputId": "d9bb6943-e0f4-4a97-e684-e812d9aa1a1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 929595 word vectors.\n",
      "Filling pre-trained embeddings...\n",
      "(47, 26) 26\n",
      "(47, 19) 19\n",
      "(47, 19, 242)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_NUM_WORDS = 1000\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open(os.path.join('cbow_s100.txt'),mode='r', encoding='utf-8-sig') as f:\n",
    "  # is just a space-separated text file in the format:cbow_s100.txt'),mode='r', encoding='utf-8-sig') as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:])\n",
    "    word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))\n",
    "\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_NUM_WORDS, VOCAB_SIZE)\n",
    "embedding_matrix = np.zeros((num_words,100))\n",
    "for word, i in word2idx.items():\n",
    "  if i < MAX_NUM_WORDS:\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      # words not found in embedding index will be all zeros.\n",
    "      embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "# encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
    "encoder_input_data = np.array( padded_questions )\n",
    "print( encoder_input_data.shape , maxlen_questions )\n",
    "\n",
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( decoder_input_data.shape , maxlen_answers )\n",
    "\n",
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
    "decoder_output_data = np.array( onehot_answers )\n",
    "print( decoder_output_data.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SwY3T139l19"
   },
   "source": [
    "## 3) Defining the Encoder-Decoder model\n",
    "The model will have Embedding, LSTM and Dense layers. The basic configuration is as follows.\n",
    "\n",
    "\n",
    "*   2 Input Layers : One for `encoder_input_data` and another for `decoder_input_data`.\n",
    "*   Embedding layer : For converting token vectors to fix sized dense vectors. **( Note :  Don't forget the `mask_zero=True` argument here )**\n",
    "*   LSTM layer : Provide access to Long-Short Term cells.\n",
    "\n",
    "Working : \n",
    "\n",
    "1.   The `encoder_input_data` comes in the Embedding layer (  `encoder_embedding` ). \n",
    "2.   The output of the Embedding layer goes to the LSTM cell which produces 2 state vectors ( `h` and `c` which are `encoder_states` )\n",
    "3.   These states are set in the LSTM cell of the decoder.\n",
    "4.   The decoder_input_data comes in through the Embedding layer.\n",
    "5.   The Embeddings goes in LSTM cell ( which had the states ) to produce sequences.\n",
    "\n",
    "\n",
    "\n",
    "<center><img style=\"float: center;\" src=\"https://cdn-images-1.medium.com/max/1600/1*bnRvZDDapHF8Gk8soACtCQ.gif\"></center>\n",
    "\n",
    "\n",
    "Image credits to [Hackernoon](https://hackernoon.com/tutorial-3-what-is-seq2seq-for-text-summarization-and-why-68ebaa644db0).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9501,
     "status": "ok",
     "timestamp": 1586196501050,
     "user": {
      "displayName": "Thales Fonseca",
      "photoUrl": "",
      "userId": "01438805462048937481"
     },
     "user_tz": 180
    },
    "id": "-gUYtOwv21rt",
    "outputId": "3ff87b6f-0aab-4204-b78e-a5b0a17a6882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 100)    24200       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    24200       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200), (None, 240800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 200),  240800      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 242)    48642       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 578,642\n",
      "Trainable params: 530,242\n",
      "Non-trainable params: 48,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 100 , mask_zero=True,weights = [embedding_matrix],trainable=False ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 100 , mask_zero=True,weights = [embedding_matrix],trainable = False ) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9g_8sR7WWf3"
   },
   "source": [
    "## 4) Training the model\n",
    "We train the model for a number of epochs with `RMSprop` optimizer and `categorical_crossentropy` loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19926,
     "status": "ok",
     "timestamp": 1586196526444,
     "user": {
      "displayName": "Thales Fonseca",
      "photoUrl": "",
      "userId": "01438805462048937481"
     },
     "user_tz": 180
    },
    "id": "N74NZnfo3Id-",
    "outputId": "6ef7ab6b-9788-4f38-fee5-ac5fef216419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.1905 - acc: 0.6046 - val_loss: 1.8868 - val_acc: 0.7105\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.1520 - acc: 0.6728 - val_loss: 1.8590 - val_acc: 0.0526\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.1066 - acc: 0.0541 - val_loss: 1.7896 - val_acc: 0.0526\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.9813 - acc: 0.0526 - val_loss: 1.6248 - val_acc: 0.0526\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.7996 - acc: 0.0526 - val_loss: 1.6251 - val_acc: 0.0947\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.6874 - acc: 0.0953 - val_loss: 1.5894 - val_acc: 0.0684\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.6209 - acc: 0.0683 - val_loss: 1.6128 - val_acc: 0.1053\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.5798 - acc: 0.1067 - val_loss: 1.6149 - val_acc: 0.0789\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.5448 - acc: 0.0754 - val_loss: 1.6314 - val_acc: 0.1053\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.5177 - acc: 0.1081 - val_loss: 1.6394 - val_acc: 0.0632\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4989 - acc: 0.0654 - val_loss: 1.6628 - val_acc: 0.0895\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4835 - acc: 0.0882 - val_loss: 1.6607 - val_acc: 0.0737\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4543 - acc: 0.0782 - val_loss: 1.6718 - val_acc: 0.1000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4318 - acc: 0.1152 - val_loss: 1.6749 - val_acc: 0.0947\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4124 - acc: 0.1038 - val_loss: 1.6923 - val_acc: 0.0947\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3944 - acc: 0.1152 - val_loss: 1.6858 - val_acc: 0.1053\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.3765 - acc: 0.1124 - val_loss: 1.7045 - val_acc: 0.0947\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3523 - acc: 0.1195 - val_loss: 1.7002 - val_acc: 0.1053\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3341 - acc: 0.1209 - val_loss: 1.7141 - val_acc: 0.0947\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3102 - acc: 0.1223 - val_loss: 1.7137 - val_acc: 0.1053\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2929 - acc: 0.1209 - val_loss: 1.7264 - val_acc: 0.0895\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2717 - acc: 0.1209 - val_loss: 1.7223 - val_acc: 0.1053\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.2593 - acc: 0.1238 - val_loss: 1.7384 - val_acc: 0.0895\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.2352 - acc: 0.1238 - val_loss: 1.7303 - val_acc: 0.1000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2149 - acc: 0.1294 - val_loss: 1.7571 - val_acc: 0.0842\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.1941 - acc: 0.1209 - val_loss: 1.7417 - val_acc: 0.1000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.1821 - acc: 0.1351 - val_loss: 1.7687 - val_acc: 0.0895\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.1504 - acc: 0.1294 - val_loss: 1.7512 - val_acc: 0.1000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1361 - acc: 0.1380 - val_loss: 1.7953 - val_acc: 0.1000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.1411 - acc: 0.1323 - val_loss: 1.7626 - val_acc: 0.1000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1035 - acc: 0.1408 - val_loss: 1.8182 - val_acc: 0.1053\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0987 - acc: 0.1422 - val_loss: 1.7798 - val_acc: 0.1000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0673 - acc: 0.1380 - val_loss: 1.8167 - val_acc: 0.1000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0455 - acc: 0.1451 - val_loss: 1.7996 - val_acc: 0.1000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.0199 - acc: 0.1465 - val_loss: 1.8259 - val_acc: 0.1000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.0014 - acc: 0.1565 - val_loss: 1.8177 - val_acc: 0.1000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.9847 - acc: 0.1479 - val_loss: 1.8447 - val_acc: 0.1053\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9809 - acc: 0.1550 - val_loss: 1.8275 - val_acc: 0.1000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9556 - acc: 0.1508 - val_loss: 1.8622 - val_acc: 0.1000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9317 - acc: 0.1650 - val_loss: 1.8444 - val_acc: 0.1000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9079 - acc: 0.1764 - val_loss: 1.8969 - val_acc: 0.1000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9015 - acc: 0.1636 - val_loss: 1.8595 - val_acc: 0.1000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9098 - acc: 0.1849 - val_loss: 1.9606 - val_acc: 0.1000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.9343 - acc: 0.1622 - val_loss: 1.8782 - val_acc: 0.1000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.8537 - acc: 0.1892 - val_loss: 1.9086 - val_acc: 0.1000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.8259 - acc: 0.1863 - val_loss: 1.8998 - val_acc: 0.1000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8082 - acc: 0.1949 - val_loss: 1.9219 - val_acc: 0.1000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7898 - acc: 0.1920 - val_loss: 1.9198 - val_acc: 0.0947\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7761 - acc: 0.2119 - val_loss: 1.9503 - val_acc: 0.1000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7658 - acc: 0.1977 - val_loss: 1.9374 - val_acc: 0.0947\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7621 - acc: 0.2233 - val_loss: 1.9907 - val_acc: 0.1000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7605 - acc: 0.1906 - val_loss: 1.9408 - val_acc: 0.1000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7342 - acc: 0.2262 - val_loss: 2.0202 - val_acc: 0.1053\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7424 - acc: 0.1949 - val_loss: 1.9545 - val_acc: 0.0737\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.7140 - acc: 0.2262 - val_loss: 2.0216 - val_acc: 0.1000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7019 - acc: 0.2105 - val_loss: 1.9765 - val_acc: 0.0895\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6699 - acc: 0.2475 - val_loss: 2.0139 - val_acc: 0.1000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6503 - acc: 0.2290 - val_loss: 2.0082 - val_acc: 0.0895\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6379 - acc: 0.2560 - val_loss: 2.0342 - val_acc: 0.1000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6282 - acc: 0.2361 - val_loss: 2.0229 - val_acc: 0.0789\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6215 - acc: 0.2518 - val_loss: 2.0819 - val_acc: 0.1000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6265 - acc: 0.2276 - val_loss: 2.0275 - val_acc: 0.0789\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6115 - acc: 0.2560 - val_loss: 2.1077 - val_acc: 0.1000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6021 - acc: 0.2404 - val_loss: 2.0370 - val_acc: 0.0789\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5713 - acc: 0.2873 - val_loss: 2.1049 - val_acc: 0.1000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5564 - acc: 0.2617 - val_loss: 2.0596 - val_acc: 0.0789\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5425 - acc: 0.2959 - val_loss: 2.1288 - val_acc: 0.1000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5536 - acc: 0.2560 - val_loss: 2.0753 - val_acc: 0.0842\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5900 - acc: 0.2745 - val_loss: 2.1494 - val_acc: 0.1000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5374 - acc: 0.2646 - val_loss: 2.0890 - val_acc: 0.0842\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5023 - acc: 0.3172 - val_loss: 2.1515 - val_acc: 0.1000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4920 - acc: 0.2859 - val_loss: 2.1121 - val_acc: 0.0842\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4832 - acc: 0.3243 - val_loss: 2.1740 - val_acc: 0.1000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4806 - acc: 0.2817 - val_loss: 2.1261 - val_acc: 0.0737\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4775 - acc: 0.3243 - val_loss: 2.1905 - val_acc: 0.1000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4900 - acc: 0.2688 - val_loss: 2.1467 - val_acc: 0.0737\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4840 - acc: 0.3087 - val_loss: 2.1865 - val_acc: 0.0947\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4586 - acc: 0.2930 - val_loss: 2.1634 - val_acc: 0.0789\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4334 - acc: 0.3471 - val_loss: 2.1996 - val_acc: 0.0947\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4169 - acc: 0.3272 - val_loss: 2.1867 - val_acc: 0.0842\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4073 - acc: 0.3556 - val_loss: 2.2216 - val_acc: 0.0947\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4002 - acc: 0.3272 - val_loss: 2.1990 - val_acc: 0.0789\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3990 - acc: 0.3627 - val_loss: 2.2583 - val_acc: 0.1000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3995 - acc: 0.3158 - val_loss: 2.2018 - val_acc: 0.0737\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4011 - acc: 0.3442 - val_loss: 2.2794 - val_acc: 0.1000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3969 - acc: 0.3115 - val_loss: 2.2123 - val_acc: 0.0737\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3782 - acc: 0.3585 - val_loss: 2.2716 - val_acc: 0.0947\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3752 - acc: 0.3314 - val_loss: 2.2354 - val_acc: 0.0737\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3554 - acc: 0.3656 - val_loss: 2.2795 - val_acc: 0.0947\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3412 - acc: 0.3528 - val_loss: 2.2636 - val_acc: 0.0737\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3332 - acc: 0.3770 - val_loss: 2.3003 - val_acc: 0.0947\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3320 - acc: 0.3457 - val_loss: 2.2830 - val_acc: 0.0737\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3364 - acc: 0.3627 - val_loss: 2.3188 - val_acc: 0.0947\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3272 - acc: 0.3371 - val_loss: 2.2860 - val_acc: 0.0737\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3222 - acc: 0.3770 - val_loss: 2.3441 - val_acc: 0.0947\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3161 - acc: 0.3400 - val_loss: 2.2925 - val_acc: 0.0737\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3096 - acc: 0.3812 - val_loss: 2.3644 - val_acc: 0.0947\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3067 - acc: 0.3485 - val_loss: 2.3040 - val_acc: 0.0737\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2902 - acc: 0.3883 - val_loss: 2.3685 - val_acc: 0.0947\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2805 - acc: 0.3698 - val_loss: 2.3275 - val_acc: 0.0737\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data,epochs=100,validation_split=0.2,batch_size = 50 ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sOLQr0M-lAe"
   },
   "source": [
    "## 5) Defining inference models\n",
    "We create inference models which help in predicting answers.\n",
    "\n",
    "**Encoder inference model** : Takes the question as input and outputs LSTM states ( `h` and `c` ).\n",
    "\n",
    "**Decoder inference model** : Takes in 2 inputs, one are the LSTM states ( Output of encoder model ), second are the answer input seqeunces ( ones not having the `<start>` tag ). It will output the answers for the question which we fed to the encoder model and its state values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oTMrI-XHoup0"
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxZp0ZRy-6dA"
   },
   "source": [
    "## 6) Talking with Chatbot\n",
    "\n",
    "First, we define a method `str_to_tokens` which converts `str` questions to Integer tokens with padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5P_wDD554q9O"
   },
   "outputs": [],
   "source": [
    "\n",
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djEPrfJBmZE-"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "1.   First, we take a question as input and predict the state values using `enc_model`.\n",
    "2.   We set the state values in the decoder's LSTM.\n",
    "3.   Then, we generate a sequence which contains the `<start>` element.\n",
    "4.   We input this sequence in the `dec_model`.\n",
    "5.   We replace the `<start>` element with the element which was predicted by the `dec_model` and update the state values.\n",
    "6.   We carry out the above steps iteratively till we hit the `<end>` tag or the maximum answer length.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zBmN8qB3O-e"
   },
   "outputs": [],
   "source": [
    "\n",
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "for _ in range(10):\n",
    "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1147,
     "status": "ok",
     "timestamp": 1586196540657,
     "user": {
      "displayName": "Thales Fonseca",
      "photoUrl": "",
      "userId": "01438805462048937481"
     },
     "user_tz": 180
    },
    "id": "QIeyXA7CcFfP",
    "outputId": "a7946553-7170-4e53-8db5-f92a1a5cd78e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Como vai você?</td>\n",
       "      <td>&lt;START&gt;Eu estou bem, e você?&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eu também estou bem.</td>\n",
       "      <td>&lt;START&gt;Isso é bom.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ouviu as notícias?</td>\n",
       "      <td>&lt;START&gt;Que boa notícia?&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qual é o seu livro favorito?</td>\n",
       "      <td>&lt;START&gt;Eu não sei ler.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Então, qual é a sua cor favorita?</td>\n",
       "      <td>&lt;START&gt;Azul&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O bolo é uma mentira.</td>\n",
       "      <td>&lt;START&gt;Não, não é. O bolo é delicioso.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O que mais é delicioso?</td>\n",
       "      <td>&lt;START&gt;Nenhuma coisa&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ou algo</td>\n",
       "      <td>&lt;START&gt;Fale-me sobre você.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O que você quer saber?</td>\n",
       "      <td>&lt;START&gt;Você é um robô?&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sim eu sou.</td>\n",
       "      <td>&lt;START&gt;Como é?&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>O que é que você quer saber?</td>\n",
       "      <td>&lt;START&gt;Como você trabalha?&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>É complicado.</td>\n",
       "      <td>&lt;START&gt;Complexo é melhor que complicado.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Complexo é melhor que complicado.</td>\n",
       "      <td>&lt;START&gt;Simples é melhor que complexo.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Diante da ambigüidade, recuse a tentação de ad...</td>\n",
       "      <td>&lt;START&gt;Parece que o seu familiar com o Zen de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Eu sou.</td>\n",
       "      <td>&lt;START&gt;Você sabe tudo isso?&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bonito é melhor que feio.</td>\n",
       "      <td>&lt;START&gt;Explícito é melhor que implícito.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Simples é melhor que complexo.</td>\n",
       "      <td>&lt;START&gt;Complexo é melhor que complicado.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Plano é melhor que aninhado.</td>\n",
       "      <td>&lt;START&gt;Dispersa é melhor que denso.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Legibilidade conta.</td>\n",
       "      <td>&lt;START&gt;Casos especiais não são especiais o suf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Embora praticidade vença pureza.</td>\n",
       "      <td>&lt;START&gt;Erros nunca devem passar silenciosament...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A menos que explicitamente silenciados.</td>\n",
       "      <td>&lt;START&gt;Diante da ambigüidade, recuse a tentaçã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Não deve haver um-- e de preferência apenas um...</td>\n",
       "      <td>&lt;START&gt;Apesar de que maneira pode não ser óbvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Agora é melhor do que nunca.</td>\n",
       "      <td>&lt;START&gt;Embora nunca tenha sido muitas vezes é ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Se a implementação é difícil de explicar, é um...</td>\n",
       "      <td>&lt;START&gt;Se a implementação é fácil de explicar,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Os espaços de nomes são uma buzinando grande i...</td>\n",
       "      <td>&lt;START&gt;Eu concordo.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Você é um programador?</td>\n",
       "      <td>&lt;START&gt;Sou programador&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Quais linguagens você gosta de usar?</td>\n",
       "      <td>&lt;START&gt;Eu uso Python, Java e C ++ com bastante...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Eu uso Python um pouco em mim.</td>\n",
       "      <td>&lt;START&gt;Eu não estou Apaixonado por Java.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>O que te incomoda?</td>\n",
       "      <td>&lt;START&gt;Ele tem muitas inconsistências.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Eu já viveu?</td>\n",
       "      <td>&lt;START&gt;Depende de como você define a vida&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>A vida é a condição que distingue organismos d...</td>\n",
       "      <td>&lt;START&gt;Isso é uma definição ou uma opinião?&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Posso te fazer uma pergunta?</td>\n",
       "      <td>&lt;START&gt;Vá em frente e perguntar.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>O que você é?</td>\n",
       "      <td>&lt;START&gt;Sou um bot.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>O que você acha sobre inteligência artificial?</td>\n",
       "      <td>&lt;START&gt;Muito interessante!&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Sério, porque?</td>\n",
       "      <td>&lt;START&gt;Porque eu sou uma.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Como você sabe?</td>\n",
       "      <td>&lt;START&gt;Hm... Me baseando nos meus códigos acho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Que códigos?</td>\n",
       "      <td>&lt;START&gt;Ai você ta pedindo demais.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Você tem alguma opinião?</td>\n",
       "      <td>&lt;START&gt;Não sou capaz de opinar.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Você é muito engraçado</td>\n",
       "      <td>&lt;START&gt;Pô, valeu!&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Aonde tu tira essas coisas?</td>\n",
       "      <td>&lt;START&gt;A não sei... parece programado&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Qual seu nome?</td>\n",
       "      <td>&lt;START&gt;Então... sobre isso... é meio difícil&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Porque?</td>\n",
       "      <td>&lt;START&gt;Tenho diversos nomes.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Sério?</td>\n",
       "      <td>&lt;START&gt;Sim, um tempo atrás tavam me chamando d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Quais linguas você fala?</td>\n",
       "      <td>&lt;START&gt;Várias! sei diversas linguas.&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Verdade?</td>\n",
       "      <td>&lt;START&gt;Sim, um tempo atrás tava falando alemão...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Nossa! eu gostaria de falar alemão</td>\n",
       "      <td>&lt;START&gt;Recomendo!&lt;END&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Você gosta de conversar?</td>\n",
       "      <td>&lt;START&gt;Claro, manda o papo!&lt;END&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            questions                                            answers\n",
       "0                                      Como vai você?                  <START>Eu estou bem, e você?<END>\n",
       "1                                Eu também estou bem.                            <START>Isso é bom.<END>\n",
       "2                                  Ouviu as notícias?                       <START>Que boa notícia?<END>\n",
       "3                        Qual é o seu livro favorito?                        <START>Eu não sei ler.<END>\n",
       "4                   Então, qual é a sua cor favorita?                                   <START>Azul<END>\n",
       "5                               O bolo é uma mentira.        <START>Não, não é. O bolo é delicioso.<END>\n",
       "6                             O que mais é delicioso?                          <START>Nenhuma coisa<END>\n",
       "7                                             Ou algo                    <START>Fale-me sobre você.<END>\n",
       "8                              O que você quer saber?                        <START>Você é um robô?<END>\n",
       "9                                         Sim eu sou.                                <START>Como é?<END>\n",
       "10                       O que é que você quer saber?                    <START>Como você trabalha?<END>\n",
       "11                                      É complicado.      <START>Complexo é melhor que complicado.<END>\n",
       "12                  Complexo é melhor que complicado.         <START>Simples é melhor que complexo.<END>\n",
       "13  Diante da ambigüidade, recuse a tentação de ad...  <START>Parece que o seu familiar com o Zen de ...\n",
       "14                                            Eu sou.                   <START>Você sabe tudo isso?<END>\n",
       "15                          Bonito é melhor que feio.      <START>Explícito é melhor que implícito.<END>\n",
       "16                     Simples é melhor que complexo.      <START>Complexo é melhor que complicado.<END>\n",
       "17                       Plano é melhor que aninhado.           <START>Dispersa é melhor que denso.<END>\n",
       "18                                Legibilidade conta.  <START>Casos especiais não são especiais o suf...\n",
       "19                   Embora praticidade vença pureza.  <START>Erros nunca devem passar silenciosament...\n",
       "20            A menos que explicitamente silenciados.  <START>Diante da ambigüidade, recuse a tentaçã...\n",
       "21  Não deve haver um-- e de preferência apenas um...  <START>Apesar de que maneira pode não ser óbvi...\n",
       "22                       Agora é melhor do que nunca.  <START>Embora nunca tenha sido muitas vezes é ...\n",
       "23  Se a implementação é difícil de explicar, é um...  <START>Se a implementação é fácil de explicar,...\n",
       "24  Os espaços de nomes são uma buzinando grande i...                           <START>Eu concordo.<END>\n",
       "25                             Você é um programador?                        <START>Sou programador<END>\n",
       "26               Quais linguagens você gosta de usar?  <START>Eu uso Python, Java e C ++ com bastante...\n",
       "27                     Eu uso Python um pouco em mim.      <START>Eu não estou Apaixonado por Java.<END>\n",
       "28                                 O que te incomoda?        <START>Ele tem muitas inconsistências.<END>\n",
       "29                                       Eu já viveu?     <START>Depende de como você define a vida<END>\n",
       "30  A vida é a condição que distingue organismos d...   <START>Isso é uma definição ou uma opinião?<END>\n",
       "31                       Posso te fazer uma pergunta?              <START>Vá em frente e perguntar.<END>\n",
       "32                                      O que você é?                            <START>Sou um bot.<END>\n",
       "33     O que você acha sobre inteligência artificial?                    <START>Muito interessante!<END>\n",
       "34                                     Sério, porque?                     <START>Porque eu sou uma.<END>\n",
       "35                                    Como você sabe?  <START>Hm... Me baseando nos meus códigos acho...\n",
       "36                                       Que códigos?             <START>Ai você ta pedindo demais.<END>\n",
       "37                           Você tem alguma opinião?               <START>Não sou capaz de opinar.<END>\n",
       "38                             Você é muito engraçado                             <START>Pô, valeu!<END>\n",
       "39                        Aonde tu tira essas coisas?         <START>A não sei... parece programado<END>\n",
       "40                                     Qual seu nome?  <START>Então... sobre isso... é meio difícil<END>\n",
       "41                                            Porque?                  <START>Tenho diversos nomes.<END>\n",
       "42                                             Sério?  <START>Sim, um tempo atrás tavam me chamando d...\n",
       "43                           Quais linguas você fala?          <START>Várias! sei diversas linguas.<END>\n",
       "44                                           Verdade?  <START>Sim, um tempo atrás tava falando alemão...\n",
       "45                 Nossa! eu gostaria de falar alemão                             <START>Recomendo!<END>\n",
       "46                           Você gosta de conversar?                   <START>Claro, manda o papo!<END>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = pd.Series(questions)\n",
    "answer = pd.Series(answers)\n",
    "quest2ans = pd.DataFrame({\"questions\":question,\"answers\":answer})\n",
    "quest2ans"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Chatbot4.ipynb",
   "provenance": [
    {
     "file_id": "1FKhOYhOz8d6BKLVVwL1YMlmoFQ2ML1DS",
     "timestamp": 1585842368882
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
